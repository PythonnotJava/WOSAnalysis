PT J
AU Bishnoi, A
   Stein, O
   Meyer, CI
   Redler, R
   Eicker, N
   Haak, H
   Hoffmann, L
   Klocke, D
   Kornblueh, L
   Suarez, E
AF Bishnoi, Abhiraj
   Stein, Olaf
   Meyer, Catrin I.
   Redler, Rene
   Eicker, Norbert
   Haak, Helmuth
   Hoffmann, Lars
   Klocke, Daniel
   Kornblueh, Luis
   Suarez, Estela
TI Earth system modeling on modular supercomputing architecture: coupled
   atmosphere-ocean simulations with ICON 2.6.6-rc
SO GEOSCIENTIFIC MODEL DEVELOPMENT
LA English
DT Article
ID CLIMATE; KILOMETER; SOFTWARE
AB The confrontation of complex Earth system model (ESM) codes with novel supercomputing architectures poses challenges to efficient modeling and job submission strategies. The modular setup of these models naturally fits a modular supercomputing architecture (MSA), which tightly integrates heterogeneous hardware resources into a larger and more flexible high-performance computing (HPC) system. While parts of the ESM codes can easily take advantage of the increased parallelism and communication capabilities of modern GPUs, others lag behind due to the long development cycles or are better suited to run on classical CPUs due to their communication and memory usage patterns. To better cope with these imbalances between the development of the model components, we performed benchmark campaigns on the Julich Wizard for European Leadership Science (JUWELS) modular HPC system. We enabled the weather and climate model Icosahedral Nonhydrostatic (ICON) to run in a coupled atmosphere-ocean setup, where the ocean and the model I/O is running on the CPU Cluster, while the atmosphere is simulated simultaneously on the GPUs of JUWELS Booster (ICON-MSA). Both atmosphere and ocean are running globally with a resolution of 5 km. In our test case, an optimal configuration in terms of model performance (core hours per simulation day) was found for the combination of 84 GPU nodes on the JUWELS Booster module to simulate the atmosphere and 80 CPU nodes on the JUWELS Cluster module, of which 63 nodes were used for the ocean simulation and the remaining 17 nodes were reserved for I/O. With this configuration the waiting times of the coupler were minimized. Compared to a simulation performed on CPUs only, the MSA approach reduces energy consumption by 45 % with comparable runtimes. ICON-MSA is able to scale up to a significant portion of the JUWELS system, making best use of the available computing resources. A maximum throughput of 170 simulation days per day (SDPD) was achieved when running ICON on 335 JUWELS Booster nodes and 268 Cluster nodes.
C1 [Bishnoi, Abhiraj; Stein, Olaf; Meyer, Catrin I.; Eicker, Norbert; Hoffmann, Lars; Suarez, Estela] Forschungszentrum Julich, Julich Supercomp Ctr, Julich, Germany.
   [Redler, Rene; Haak, Helmuth; Klocke, Daniel; Kornblueh, Luis] Max Planck Inst Meteorol, Hamburg, Germany.
   [Eicker, Norbert] Univ Wuppertal, Sch Math & Nat Sci, Wuppertal, Germany.
   [Suarez, Estela] Univ Bonn, Inst Comp Sci, Bonn, Germany.
   [Bishnoi, Abhiraj] Kalkulo AS, Oslo, Norway.
C3 Helmholtz Association; Research Center Julich; Max Planck Society;
   University of Wuppertal; University of Bonn
RP Bishnoi, A; Stein, O (通讯作者)，Forschungszentrum Julich, Julich Supercomp Ctr, Julich, Germany.
EM abhirajbishnoi@yahoo.in; o.stein@fz-juelich.de
RI Meyer, Catrin/B-7094-2013; Hoffmann, Lars/ABI-3746-2020; Suarez,
   Estela/J-5939-2012
OI Suarez, Estela/0000-0003-0748-7264; Meyer, Catrin/0000-0002-9271-6174;
   Hoffmann, Lars/0000-0003-3773-4377
FU Helmholtz Association of German Research Centers (HGF) through the Joint
   Lab Exascale Earth System Modeling (JL-ExaESM); AIDAS project of the
   Forschungszentrum Julich; CEA
FX This work was supported by the Helmholtz Association of German Research
   Centers (HGF) through the Joint Lab Exascale Earth System Modeling
   (JL-ExaESM) activity and by the AIDAS project of the Forschungszentrum
   Julich and CEA. The authors gratefully acknowledge the computing time
   and storage resources granted through the ESM partition on the
   supercomputer JUWELS at the Juelich Supercomputing Centre,
   Forschungszentrum Juelich, Germany. We are thankful to Dmitry Alexeev,
   NVIDIA, Will Sawyer, CSCS, PRACE project pra127, and the application
   support and systems operations teams at JSC.
NR 36
TC 0
Z9 0
U1 1
U2 4
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLEE 1E, GOTTINGEN, 37081, GERMANY
SN 1991-959X
EI 1991-9603
J9 GEOSCI MODEL DEV
JI Geosci. Model Dev.
PD JAN 12
PY 2024
VL 17
IS 1
BP 261
EP 273
DI 10.5194/gmd-17-261-2024
PG 13
WC Geosciences, Multidisciplinary
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Geology
GA IL8Q2
UT WOS:001166577100001
OA gold
DA 2025-05-29
ER
